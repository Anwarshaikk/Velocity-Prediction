{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2ac4df-f3d3-4d35-8ddb-a8d67f129c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities excel file saved to /Users/vivek/Velocity Prediction/Segmented_Probabilities.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "import os\n",
    "\n",
    "# Load the dataset\n",
    "base_dir = os.getcwd()\n",
    "drive_cycle_file_path = os.path.join(base_dir, 'Combined_Preprocessed_Dataset.xlsx')\n",
    "data = pd.read_excel(drive_cycle_file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "speed_data = data['Target Speed, mph'].values\n",
    "\n",
    "# Step 1: Segment data into overlapping time windows\n",
    "def segment_by_time_window(speed_data, segment_length=10):\n",
    "    \"\"\"\n",
    "    Segments the speed data into overlapping windows of specified length.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    for i in range(len(speed_data) - segment_length + 1):\n",
    "        segments.append(speed_data[i:i + segment_length])\n",
    "    return segments\n",
    "\n",
    "# Step 2: Categorize segments\n",
    "def categorize_segment(segment_speed, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Categorizes the segment as Acceleration, Deceleration, Constant Speed, or Fluctuating.\n",
    "    \"\"\"\n",
    "    diffs = np.diff(segment_speed)\n",
    "    avg_diff = np.mean(diffs)  # Average difference\n",
    "    total_values = len(diffs)\n",
    "    \n",
    "    positive_counts = np.sum(diffs > 0)\n",
    "    negative_counts = np.sum(diffs < 0)\n",
    "    zero_counts = np.sum(diffs == 0)\n",
    "\n",
    "    # Calculate proportions\n",
    "    positive_ratio = positive_counts / total_values\n",
    "    negative_ratio = negative_counts / total_values\n",
    "    zero_ratio = zero_counts / total_values\n",
    "    \n",
    "    # Categorization based on avg_diff and proportions\n",
    "    if avg_diff > threshold and positive_ratio > max(negative_ratio, zero_ratio):\n",
    "        return 'Acceleration'\n",
    "    elif avg_diff < -threshold and negative_ratio > max(positive_ratio, zero_ratio):\n",
    "        return 'Deceleration'\n",
    "    elif abs(avg_diff) <= threshold and zero_ratio >= 0.5:\n",
    "        return 'Constant Speed'\n",
    "    else:\n",
    "        return 'Fluctuating'\n",
    "\n",
    "# Step 3: Calculate conditional probabilities\n",
    "def calculate_conditional_probabilities(segments, categories, segment_length):\n",
    "    \"\"\"\n",
    "    Calculates conditional probabilities for each category in every segment.\n",
    "    \"\"\"\n",
    "    probabilities = []\n",
    "    for i, segment in enumerate(segments):\n",
    "        diffs = np.diff(segment)\n",
    "        total_points = len(diffs)\n",
    "\n",
    "        # Total counts for acceleration, deceleration, constant speed, and fluctuating\n",
    "        positive_counts = np.sum(diffs > 0)\n",
    "        negative_counts = np.sum(diffs < 0)\n",
    "        zero_counts = np.sum(diffs == 0)\n",
    "\n",
    "        prob_acceleration = positive_counts / total_points\n",
    "        prob_deceleration = negative_counts / total_points\n",
    "        prob_constant_speed = zero_counts / total_points\n",
    "        probabilities.append({\n",
    "            'Segment': ';'.join(map(str, segment)),\n",
    "            'Category': categories[i],\n",
    "            'Prob_Acceleration': prob_acceleration,\n",
    "            'Prob_Deceleration': prob_deceleration,\n",
    "            'Prob_Constant_Speed': prob_constant_speed\n",
    "        })\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# Segment data\n",
    "segments = segment_by_time_window(speed_data, segment_length=10)\n",
    "\n",
    "# Categorize each segment\n",
    "categories = [categorize_segment(segment) for segment in segments]\n",
    "\n",
    "# Calculate conditional probabilities\n",
    "probabilities = calculate_conditional_probabilities(segments, categories, segment_length=10)\n",
    "\n",
    "# Save to Excel\n",
    "output_df = pd.DataFrame(probabilities)\n",
    "output_file_path = os.path.join(base_dir, 'Segmented_Probabilities.xlsx')\n",
    "output_df.to_excel(output_file_path, index=False)\n",
    "print(f\"Conditional probabilities excel file saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e5a453c-1335-4893-8385-bd7d16c6c717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 436us/step - loss: 0.0868 - val_loss: 0.0837\n",
      "Epoch 2/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 435us/step - loss: 0.0835 - val_loss: 0.0836\n",
      "Epoch 3/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 437us/step - loss: 0.0836 - val_loss: 0.0835\n",
      "Epoch 4/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 442us/step - loss: 0.0833 - val_loss: 0.0836\n",
      "Epoch 5/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 440us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 6/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 441us/step - loss: 0.0834 - val_loss: 0.0835\n",
      "Epoch 7/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 457us/step - loss: 0.0832 - val_loss: 0.0835\n",
      "Epoch 8/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 445us/step - loss: 0.0834 - val_loss: 0.0835\n",
      "Epoch 9/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 442us/step - loss: 0.0831 - val_loss: 0.0835\n",
      "Epoch 10/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 443us/step - loss: 0.0835 - val_loss: 0.0835\n",
      "Epoch 11/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 444us/step - loss: 0.0834 - val_loss: 0.0836\n",
      "Epoch 12/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 445us/step - loss: 0.0832 - val_loss: 0.0835\n",
      "Epoch 13/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 459us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 14/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 446us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 15/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 452us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 16/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 450us/step - loss: 0.0834 - val_loss: 0.0835\n",
      "Epoch 17/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 450us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 18/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 448us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 19/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 444us/step - loss: 0.0832 - val_loss: 0.0836\n",
      "Epoch 20/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 451us/step - loss: 0.0834 - val_loss: 0.0836\n",
      "Epoch 21/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 454us/step - loss: 0.0830 - val_loss: 0.0835\n",
      "Epoch 22/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 446us/step - loss: 0.0834 - val_loss: 0.0835\n",
      "Epoch 23/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 461us/step - loss: 0.0831 - val_loss: 0.0836\n",
      "Epoch 24/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 461us/step - loss: 0.0836 - val_loss: 0.0836\n",
      "Epoch 25/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 459us/step - loss: 0.0832 - val_loss: 0.0837\n",
      "Epoch 26/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 455us/step - loss: 0.0833 - val_loss: 0.0838\n",
      "Epoch 27/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 461us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 28/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 458us/step - loss: 0.0834 - val_loss: 0.0835\n",
      "Epoch 29/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 464us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 30/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 467us/step - loss: 0.0835 - val_loss: 0.0835\n",
      "Epoch 31/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 454us/step - loss: 0.0835 - val_loss: 0.0835\n",
      "Epoch 32/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 452us/step - loss: 0.0835 - val_loss: 0.0835\n",
      "Epoch 33/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 456us/step - loss: 0.0834 - val_loss: 0.0836\n",
      "Epoch 34/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 452us/step - loss: 0.0835 - val_loss: 0.0835\n",
      "Epoch 35/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 451us/step - loss: 0.0834 - val_loss: 0.0836\n",
      "Epoch 36/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 490us/step - loss: 0.0835 - val_loss: 0.0835\n",
      "Epoch 37/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 466us/step - loss: 0.0834 - val_loss: 0.0836\n",
      "Epoch 38/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 455us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 39/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 453us/step - loss: 0.0832 - val_loss: 0.0835\n",
      "Epoch 40/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 454us/step - loss: 0.0832 - val_loss: 0.0836\n",
      "Epoch 41/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 481us/step - loss: 0.0832 - val_loss: 0.0835\n",
      "Epoch 42/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 486us/step - loss: 0.0832 - val_loss: 0.0835\n",
      "Epoch 43/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 488us/step - loss: 0.0832 - val_loss: 0.0838\n",
      "Epoch 44/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 501us/step - loss: 0.0834 - val_loss: 0.0835\n",
      "Epoch 45/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 498us/step - loss: 0.0834 - val_loss: 0.0835\n",
      "Epoch 46/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 474us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 47/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 475us/step - loss: 0.0833 - val_loss: 0.0835\n",
      "Epoch 48/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 479us/step - loss: 0.0834 - val_loss: 0.0836\n",
      "Epoch 49/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 475us/step - loss: 0.0834 - val_loss: 0.0836\n",
      "Epoch 50/50\n",
      "\u001b[1m18094/18094\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 472us/step - loss: 0.0832 - val_loss: 0.0835\n",
      "\u001b[1m2262/2262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204us/step - loss: 0.0831\n",
      "Model Loss: 0.08354642987251282\n",
      "Predicted velocities saved to /Users/vivek/Velocity Prediction/Predicted_Velocities_50s.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import os\n",
    "\n",
    "# Step 1: Load Probabilities Excel\n",
    "base_dir = os.getcwd()\n",
    "input_path = os.path.join(base_dir, 'Segmented_Probabilities.xlsx')\n",
    "data = pd.read_excel(input_path)\n",
    "\n",
    "# Step 2: Prepare Features and Labels\n",
    "X = data[['Prob_Acceleration', 'Prob_Deceleration', 'Prob_Constant_Speed']].values\n",
    "\n",
    "# Generate target velocities for the next 50 seconds (replace with real data if available)\n",
    "# For demonstration, using random values as placeholders.\n",
    "y = np.random.random(len(data))  # Placeholder for actual next velocities.\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Build and Train LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Reshape input for LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Model Loss: {loss}\")\n",
    "\n",
    "# Step 6: Predict Future Velocities\n",
    "future_timestamps = 100  # Number of seconds to predict\n",
    "predicted_velocities = []\n",
    "\n",
    "# Generate predictions step-by-step for 50 seconds\n",
    "input_sequence = X_test[0]  # Initial input sequence (shape: 3 features)\n",
    "input_sequence = input_sequence.reshape(1, input_sequence.shape[0], 1)  # Reshape for LSTM input\n",
    "for i in range(future_timestamps):\n",
    "    # Predict next velocity\n",
    "    pred_velocity = model.predict(input_sequence, verbose=0)\n",
    "    predicted_velocities.append(pred_velocity[0, 0])\n",
    "\n",
    "    # Update input sequence for the next step\n",
    "    input_sequence = np.roll(input_sequence, -1, axis=1)  # Roll the sequence\n",
    "    input_sequence[0, -1, 0] = pred_velocity[0, 0]  # Update with predicted velocity\n",
    "\n",
    "# Step 7: Save Predicted Velocities to Excel\n",
    "timestamp_count = list(range(1, future_timestamps + 1))  # Timestamps: 1 to 50\n",
    "results_df = pd.DataFrame({\n",
    "    'Timestamp': timestamp_count,\n",
    "    'Predicted Velocity': predicted_velocities\n",
    "})\n",
    "\n",
    "output_path = os.path.join(base_dir, 'Predicted_Velocities_50s.xlsx')\n",
    "results_df.to_excel(output_path, index=False)\n",
    "print(f\"Predicted velocities saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bbb6dbc-a640-4d65-8257-5976cd7ab46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conditional probabilities saved to /Users/vivek/Velocity Prediction/Segmented_Probabilities.xlsx\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Load the dataset\n",
    "import os\n",
    "base_dir = os.getcwd()\n",
    "drive_cycle_file_path = base_dir + '/Combined_Preprocessed_Dataset.xlsx'\n",
    "data = pd.read_excel(drive_cycle_file_path)\n",
    "\n",
    "# Extract relevant columns\n",
    "speed_data = data['Target Speed, mph'].values\n",
    "\n",
    "# Step 1: Segment data into overlapping time windows\n",
    "def segment_by_time_window(speed_data, segment_length=10):\n",
    "    \"\"\"\n",
    "    Segments the speed data into overlapping windows of specified length.\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    for i in range(len(speed_data) - segment_length + 1):\n",
    "        segments.append(speed_data[i:i + segment_length])\n",
    "    return segments\n",
    "\n",
    "# Step 2: Categorize segments\n",
    "def categorize_segment(segment_speed, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Categorizes the segment as Acceleration, Deceleration, Constant Speed, or Fluctuating.\n",
    "    \"\"\"\n",
    "    diffs = np.diff(segment_speed)\n",
    "    avg_diff = np.mean(diffs)  # Average difference\n",
    "    total_values = len(diffs)\n",
    "    \n",
    "    positive_counts = np.sum(diffs > 0)\n",
    "    negative_counts = np.sum(diffs < 0)\n",
    "    zero_counts = np.sum(diffs == 0)\n",
    "\n",
    "    # Calculate proportions\n",
    "    positive_ratio = positive_counts / total_values\n",
    "    negative_ratio = negative_counts / total_values\n",
    "    zero_ratio = zero_counts / total_values\n",
    "    \n",
    "    # Categorization based on avg_diff and proportions\n",
    "    if avg_diff > threshold and positive_ratio > max(negative_ratio, zero_ratio):\n",
    "        return 'Acceleration'\n",
    "    elif avg_diff < -threshold and negative_ratio > max(positive_ratio, zero_ratio):\n",
    "        return 'Deceleration'\n",
    "    elif abs(avg_diff) <= threshold and zero_ratio >= 0.5:\n",
    "        return 'Constant Speed'\n",
    "    else:\n",
    "        return 'Fluctuating'\n",
    "\n",
    "# Step 3: Calculate conditional probabilities\n",
    "def calculate_conditional_probabilities(segments, categories, segment_length):\n",
    "    \"\"\"\n",
    "    Calculates conditional probabilities for each category in every segment.\n",
    "    \"\"\"\n",
    "    probabilities = []\n",
    "    for i, segment in enumerate(segments):\n",
    "        category = categories[i]\n",
    "        diffs = np.diff(segment)\n",
    "        \n",
    "        # Total counts for acceleration, deceleration, and constant speed\n",
    "        positive_counts = np.sum(diffs > 0)\n",
    "        negative_counts = np.sum(diffs < 0)\n",
    "        zero_counts = np.sum(diffs == 0)\n",
    "        total_counts = len(diffs)\n",
    "        \n",
    "        # Conditional probabilities\n",
    "        prob_acceleration = positive_counts / total_counts\n",
    "        prob_deceleration = negative_counts / total_counts\n",
    "        prob_constant_speed = zero_counts / total_counts\n",
    "\n",
    "        probabilities.append({\n",
    "            'Segment': ';'.join(map(str, segment)),\n",
    "            'Category': category,\n",
    "            'Prob_Acceleration': prob_acceleration,\n",
    "            'Prob_Deceleration': prob_deceleration,\n",
    "            'Prob_Constant_Speed': prob_constant_speed\n",
    "        })\n",
    "\n",
    "    return probabilities\n",
    "\n",
    "# Segment data\n",
    "segments = segment_by_time_window(speed_data, segment_length=10)\n",
    "\n",
    "# Categorize each segment\n",
    "categories = [categorize_segment(segment) for segment in segments]\n",
    "\n",
    "# Calculate conditional probabilities\n",
    "probabilities = calculate_conditional_probabilities(segments, categories, segment_length=10)\n",
    "\n",
    "# Save to Excel\n",
    "output_df = pd.DataFrame(probabilities)\n",
    "output_file_path = base_dir + '/Segmented_Probabilities.xlsx'\n",
    "output_df.to_excel(output_file_path, index=False)\n",
    "print(f\"Conditional probabilities saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725732f9-0a48-47c0-8a9a-159dcf882ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
